{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "import uuid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime as dt, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from src.misc import *\n",
    "from src.io_utils import write_image, concatenate_rasters\n",
    "from src.cloud_detector import CloudDetector\n",
    "from src.datasets import load_landslide_df\n",
    "from empatches import EMPatches\n",
    "\n",
    "# geospatial packages\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from shapely.geometry import shape\n",
    "import fiona\n",
    "import richdem\n",
    "from pyproj import CRS\n",
    "\n",
    "# planetary computer and related\n",
    "import planetary_computer as pc\n",
    "import pystac_client as stac\n",
    "import stackstac as ss\n",
    "import distributed as dask\n",
    "import rasterio as rio\n",
    "import rioxarray as rxr\n",
    "\n",
    "IMAGE_FOLDER = 'indonesia'\n",
    "S2_NATIVE_CRS = 32750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read inventory\n",
    "gdf = gpd.read_file('inventories/indonesia_landslides.gpkg')\n",
    "\n",
    "# infer mapping extent (as a convex hull)\n",
    "aoi = gdf.union_all().convex_hull\n",
    "# buffer it by at least 500m in each direction\n",
    "buffer = max(find_buffer_values(aoi, meters=500))\n",
    "aoi = aoi.buffer(buffer, cap_style='square', join_style='mitre', mitre_limit=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Search & download S2 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the STAC catalog\n",
    "catalog = stac.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier = pc.sign_inplace,\n",
    ")\n",
    "\n",
    "# get only products from 90 days before to 30 days after activation date\n",
    "event_date = gdf['event_date'].iloc[0]\n",
    "min_pre_date = (event_date + timedelta(days=-90)).strftime('%Y-%m-%d')\n",
    "max_pre_date = (event_date + timedelta(seconds=-1)).strftime('%Y-%m-%d')\n",
    "min_post_date = event_date.strftime('%Y-%m-%d')\n",
    "max_post_date = (event_date + timedelta(days=30, seconds=-1)).strftime('%Y-%m-%d')\n",
    "\n",
    "# define queries\n",
    "search = {\n",
    "    \"pre_s2\"  :  {\n",
    "        \"collections\" :  [\"sentinel-2-l2a\"],\n",
    "        \"intersects\"  :  aoi,\n",
    "        \"datetime\"    :  f\"{min_pre_date}/{max_pre_date}\",\n",
    "        \"query\"       :  {\"eo:cloud_cover\": {\"lt\": 90}}\n",
    "    },\n",
    "    \"post_s2\"  :  {\n",
    "        \"collections\" :  [\"sentinel-2-l2a\"],\n",
    "        \"intersects\"  :  aoi,\n",
    "        \"datetime\"    :  f\"{min_post_date}/{max_post_date}\",\n",
    "        \"query\"       :  {\"eo:cloud_cover\": {\"lt\": 90}}\n",
    "    },\n",
    "    \"dem\"  :  {\n",
    "        \"collections\" :  [\"alos-dem\"],\n",
    "        \"intersects\"  :  aoi,\n",
    "        \"datetime\"    :  \"2016-12-07/2016-12-08\",\n",
    "        \"query\"       :  {}\n",
    "    },\n",
    "}\n",
    "\n",
    "# execute queries\n",
    "items = {}\n",
    "for k in search.keys():\n",
    "    items[k] = catalog.search(**search[k]).item_collection()\n",
    "    print(f\"[{k.upper()}]: {len(items[k])}\", end=\" | \" if k != \"dem\" else \"\\n\", )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Dask client\n",
    "client = dask.Client(processes=False)\n",
    "print(f\"Dask dashboard: {client.dashboard_link}\")\n",
    "\n",
    "assets = {\n",
    "    's2' : ['AOT', 'B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B09', 'B11', 'B12', 'B8A', 'SCL', 'WVP']\n",
    "}\n",
    "# NB: 'visual' and 'rendered_preview' cannot be included because they are multi-band assets\n",
    "# rio.to_raster() can only handle 1-band assets\n",
    "\n",
    "# download the corresponding Sentinel-2 products\n",
    "print(f\"Downloading products...\")\n",
    "for k in items.keys():\n",
    "    \n",
    "    sat = k.split('_')[-1]\n",
    "\n",
    "    # select collection of products for landslide i and date-range_satellite k\n",
    "    item_collection = items[k]\n",
    "\n",
    "    if sat in ['s1', 's2']:\n",
    "\n",
    "        # download products\n",
    "        # NB: 2+ products with the same date are really contiguous tiles of the\n",
    "        # same product, so when referring to an 'item' here we are actually\n",
    "        # referring to an item collection (1+) of contiguous tiles. So we need\n",
    "        # to split the item collection by date.\n",
    "        for item in tqdm(split_item_collection_by_date(item_collection), desc=k):\n",
    "\n",
    "            acquisition_date = item[0].datetime.strftime('%Y-%m-%dT%H%M%SZ')\n",
    "            #epsg = item[0].properties['proj:epsg']  # set CRS to the first contiguous tile (for disambiguation)\n",
    "\n",
    "            orbit_state = ''\n",
    "            if sat == 's1':\n",
    "                orbit_state = '_asc' if item[0].properties['sat:orbit_state'] == 'ascending' else '_desc'\n",
    "                \n",
    "            cloud_cover = ''\n",
    "            if sat == 's2':\n",
    "                cloud_cover = f\"_cc{round(item[0].properties['eo:cloud_cover'])}\"\n",
    "            \n",
    "            # set output filename\n",
    "            fn = f\"images/{IMAGE_FOLDER}/{k.split('_')[0]}/{acquisition_date}{orbit_state}{cloud_cover}.tif\"\n",
    "            if os.path.exists(fn): continue\n",
    "            os.makedirs(os.path.dirname(fn), exist_ok=True)\n",
    "\n",
    "            # cast item from pystac.item.Item to xarray.DataArray\n",
    "            s = ss.stack(\n",
    "                item,\n",
    "                bounds_latlon = aoi.bounds,\n",
    "                assets = assets[sat],\n",
    "                resolution = 10,\n",
    "                epsg = S2_NATIVE_CRS,\n",
    "                xy_coords = 'center',\n",
    "                #dtype = np.uint16, # set later\n",
    "                #fill_value = 0, # later we will set nodata value as 0\n",
    "                )\n",
    "            \n",
    "            # if processing baseline >= 4 (introduced in 25 january 2022),\n",
    "            # harmonize data to the old processing baseline\n",
    "            s = harmonize_to_old_processing_baseline(s)\n",
    "            \n",
    "            # cast item from xarray.DataArray to xarray.Dataset\n",
    "            #s = s.to_dataset(dim='band')\n",
    "\n",
    "            # cast SCL to uint8 (useless when rasterizing, because\n",
    "            # rio.to_raster() saves every band with the dtype of the first band)\n",
    "            #s['SCL'] = s['SCL'].astype('uint8')\n",
    "\n",
    "            # stack eventual overlapping tiles by performing a median composite\n",
    "            # in the time dimension (ignore NaNs)\n",
    "            s = s.median(dim='time', keep_attrs=True, skipna=True)\n",
    "            \n",
    "            # cast to uint16 (this also fills NaNs with 0)\n",
    "            s = s.round().astype(np.uint16)\n",
    "\n",
    "            # set nodata value as 0\n",
    "            s = s.rio.write_nodata(0)\n",
    "\n",
    "            # save to geotiff\n",
    "            s.rio.to_raster(fn, compress='lzw')\n",
    "\n",
    "            # change band names\n",
    "            # (this is needed because rio.to_raster doesn't save band names in\n",
    "            # the geotiff by default)\n",
    "            with rio.open(fn, 'r+') as ds:\n",
    "                ds.descriptions = assets[sat]\n",
    "            \n",
    "            # NB: another option would be to convert the xarray.DataArray to a xarray.Dataset:\n",
    "            # s = s.to_dataset(\"band\")\n",
    "            # and then drop the band dimension (as bands are now variables):\n",
    "            # s = s.squeeze(\"band\", drop=True)\n",
    "                \n",
    "    elif sat == 'dem':\n",
    "        for item in tqdm(split_item_collection_by_date(item_collection), desc=k):\n",
    "\n",
    "            # set output filename\n",
    "            fn = f\"images/{IMAGE_FOLDER}/dem_30m.tif\"\n",
    "            if os.path.exists(fn): continue\n",
    "            os.makedirs(os.path.dirname(fn), exist_ok=True)\n",
    "            \n",
    "            # cast item from pystac.item.Item to xarray.DataArray\n",
    "            s = ss.stack(\n",
    "                item,\n",
    "                bounds_latlon = aoi.bounds,\n",
    "                assets = ['data'],\n",
    "                #resolution = 10,   # USE DEFAULT AND RESIZE AFTER COMPUTING SLOPE AND ASPECT\n",
    "                epsg = S2_NATIVE_CRS,\n",
    "                xy_coords = 'center',\n",
    "                )\n",
    "            \n",
    "            # stack eventual overlapping tiles by performing a median composite\n",
    "            # in the time dimension\n",
    "            s = s.median(dim='time', keep_attrs=True, skipna=True)\n",
    "\n",
    "            # fill NaNs with -9999\n",
    "            s = s.fillna(-9999)\n",
    "            \n",
    "            # cast to int\n",
    "            s = s.round().astype(int)\n",
    "\n",
    "            # set nodata value as -9999\n",
    "            s = s.rio.write_nodata(-9999)\n",
    "\n",
    "            # save to geotiff\n",
    "            s.rio.to_raster(fn, compress='lzw')\n",
    "\n",
    "            # change band names\n",
    "            with rio.open(fn, 'r+') as ds:\n",
    "                ds.descriptions = ['data']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cloud detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_detector = CloudDetector(device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "files = nglob(f'images/{IMAGE_FOLDER}/*/*.tif')\n",
    "\n",
    "for fn in tqdm(files):\n",
    "\n",
    "    # detect clouds\n",
    "    with rxr.open_rasterio(fn) as stack:\n",
    "        if 'cloud_mask' in stack.long_name: continue\n",
    "        mask = cloud_detector(stack)\n",
    "\n",
    "    # append cloud mask to the raster\n",
    "    temp_fn = f'.temp/{str(uuid.uuid4().hex)}.tif'\n",
    "    write_image(mask, temp_fn, georef_like=fn, compress='lzw', descriptions=['cloud_mask'])\n",
    "    concatenate_rasters([fn,temp_fn], fn)\n",
    "    os.remove(temp_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate slope and aspect from DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.earthdatascience.org/tutorials/get-slope-aspect-from-digital-elevation-model/\n",
    "\n",
    "dem_xr = rxr.open_rasterio(f'images/{IMAGE_FOLDER}/dem_30m.tif')\n",
    "dem = dem_xr.data[0]    # to numpy array\n",
    "dem = richdem.rdarray(dem, no_data=-9999)   # to richdem array\n",
    "\n",
    "# get geotransform matrix in the richdem format\n",
    "# GT(0) x-coordinate of the upper-left corner of the upper-left pixel.\n",
    "# GT(1) w-e pixel resolution / pixel width.\n",
    "# GT(2) row rotation (typically zero).\n",
    "# GT(3) y-coordinate of the upper-left corner of the upper-left pixel.\n",
    "# GT(4) column rotation (typically zero).\n",
    "# GT(5) n-s pixel resolution / pixel height (negative value for a north-up image).\n",
    "with rio.open(f'images/{IMAGE_FOLDER}/dem_30m.tif') as ds:\n",
    "    gtr = list(ds.transform)\n",
    "    gtr = (gtr[2], gtr[0], 0, gtr[5], 0, gtr[4])\n",
    "    proj = CRS.from_epsg(S2_NATIVE_CRS).to_proj4()\n",
    "\n",
    "# set richdem array's geotransform matrix and projection (needed for slope and aspect computation)\n",
    "dem.geotransform = gtr\n",
    "dem.projection = proj\n",
    "\n",
    "# compute slope and aspect\n",
    "slope = richdem.TerrainAttribute(dem, attrib='slope_degrees')\n",
    "aspect = richdem.TerrainAttribute(dem, attrib='aspect')\n",
    "# other DEM-derivatives to consider for landslide mapping:\n",
    "# https://link.springer.com/article/10.1007/s11069-018-3543-1\n",
    "\n",
    "# to rioxarray's RasterArray\n",
    "slope_xr = dem_xr.copy(deep=True, data=np.array(slope)[np.newaxis, :, :])\n",
    "aspect_xr = dem_xr.copy(deep=True, data=np.array(aspect)[np.newaxis, :, :])\n",
    "\n",
    "# resize to the same grid of S2 images\n",
    "template_path = nglob(f'images/{IMAGE_FOLDER}/pre/*')[0]\n",
    "template_xr = rxr.open_rasterio(template_path)\n",
    "dem_xr = dem_xr.rio.reproject_match(template_xr, resampling=rio.enums.Resampling.bilinear, nodata=-9999)\n",
    "slope_xr = slope_xr.rio.reproject_match(template_xr, resampling=rio.enums.Resampling.bilinear, nodata=-1)\n",
    "aspect_xr = aspect_xr.rio.reproject_match(template_xr, resampling=rio.enums.Resampling.nearest, nodata=-1)\n",
    "\n",
    "# interpolate missing values (may be caused by reprojection)\n",
    "dem_xr = dem_xr.rio.interpolate_na('nearest')\n",
    "slope_xr = slope_xr.rio.interpolate_na('nearest')\n",
    "aspect_xr = aspect_xr.rio.interpolate_na('nearest')\n",
    "\n",
    "# save to geotiff\n",
    "dem_xr.rio.to_raster(f'images/{IMAGE_FOLDER}/dem.tif', compress='lzw')\n",
    "slope_xr.rio.to_raster(f'images/{IMAGE_FOLDER}/slope.tif', compress='lzw')\n",
    "aspect_xr.rio.to_raster(f'images/{IMAGE_FOLDER}/aspect.tif', compress='lzw')\n",
    "\n",
    "# visualize\n",
    "#richdem.rdShow(slope, axes=False, cmap='magma', figsize=(8, 6))\n",
    "#richdem.rdShow(aspect, axes=False, cmap='jet', figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Rasterize landslide polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change CRS to S2_NATIVE_CRS\n",
    "gdf = gdf.to_crs(epsg=S2_NATIVE_CRS)\n",
    "\n",
    "# Get list of landslide polygons\n",
    "geom = gdf.geometry.to_list()\n",
    "\n",
    "# Read metadata from template raster\n",
    "template_path = nglob(f'images/{IMAGE_FOLDER}/pre/*.tif')[0]\n",
    "with rio.open(template_path) as template:\n",
    "    template_shape = template.shape\n",
    "    template_transform = template.transform\n",
    "\n",
    "# Rasterize vector using the shape and coordinate system of the template raster\n",
    "#https://github.com/rasterio/rasterio/blob/33a8c9436f47447de333b843f9565d3586983dd2/rasterio/features.py#L182\n",
    "rasterized_alltouched = rio.features.rasterize(\n",
    "    geom,\n",
    "    out_shape = template_shape,\n",
    "    fill = 0,\n",
    "    transform = template_transform,\n",
    "    all_touched = True, # to avoid gaps between contiguous landslides (since they compete for the same cell)\n",
    "    default_value = 255,\n",
    "    dtype = np.uint8\n",
    ")[np.newaxis,...]\n",
    "\n",
    "rasterized = rio.features.rasterize(\n",
    "    geom,\n",
    "    out_shape = template_shape,\n",
    "    fill = 0,\n",
    "    transform = template_transform,\n",
    "    all_touched = False, # no avoidance of gaps between contiguous landslides\n",
    "    default_value = 255,\n",
    "    dtype = np.uint8\n",
    ")[np.newaxis,...]\n",
    "\n",
    "# Save rasterized vector\n",
    "write_image(\n",
    "    rasterized_alltouched,\n",
    "    path = f\"images/{IMAGE_FOLDER}/polygons_alltouched.tif\",\n",
    "    georef_like = template_path,\n",
    "    compress = 'lzw'\n",
    ")\n",
    "\n",
    "write_image(\n",
    "    rasterized,\n",
    "    path = f\"images/{IMAGE_FOLDER}/polygons.tif\",\n",
    "    georef_like = template_path,\n",
    "    compress = 'lzw'\n",
    ")\n",
    "\n",
    "# change band name\n",
    "with rio.open(f\"images/{IMAGE_FOLDER}/polygons_alltouched.tif\", 'r+') as ds:\n",
    "    ds.descriptions = ('data',)\n",
    "\n",
    "with rio.open(f\"images/{IMAGE_FOLDER}/polygons.tif\", 'r+') as ds:\n",
    "    ds.descriptions = ('data',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Divide into patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCHSIZE = 256\n",
    "STRIDE = 128\n",
    "VISIBLE_LANDSLIDE_PIXELS_THRESHOLD = 1\n",
    "NOT_VISIBLE_CLASSES = [1,2] # 0 = clear sky, 1 = thick clouds, 2 = thin clouds/fog/smoke, 3 = cloud shadow\n",
    "\n",
    "# load the landslide dataframe\n",
    "s2_df = load_landslide_df(f'images/{IMAGE_FOLDER}')\n",
    "s2_pre_df = s2_df[s2_df['time'] == 'pre']\n",
    "s2_post_df = s2_df[s2_df['time'] == 'post']\n",
    "\n",
    "# read rasterized polygons\n",
    "with rio.open(f'images/{IMAGE_FOLDER}/polygons.tif') as polygons:\n",
    "    polygons_crs = polygons.crs\n",
    "    polygons_transform = polygons.transform\n",
    "    polygons_img = polygons.read(1)\n",
    "\n",
    "# NB: padding is not performed here, so the patches on the lower and right edges\n",
    "# of the image may overlap more than the specified stride with the penultimate\n",
    "# patches of the same edges\n",
    "\n",
    "# extract patches (p_idx is in (h_start, h_end, w_start, w_end) format)\n",
    "polygons_p, polygons_p_idx = EMPatches().extract_patches(polygons_img, patchsize=256, stride=128)\n",
    "\n",
    "# keep patches that contain at least VISIBLE_LANDSLIDE_PIXELS_THRESHOLD landslide pixel\n",
    "polygons_p_idx_with_vis_ls = []\n",
    "fn_pre_list = []\n",
    "fn_post_list = []\n",
    "n_vis_ls_pixels_list = []\n",
    "perc_cloudy_pixels_list = []\n",
    "\n",
    "for p, p_idx in tqdm(zip(polygons_p, polygons_p_idx), total=len(polygons_p_idx)):\n",
    "    h_start, h_end, w_start, w_end = p_idx\n",
    "\n",
    "    # discard patch if it does not contain any landslide pixels\n",
    "    ls_mask = (p == 255)\n",
    "    n_ls_pixels = np.sum(ls_mask)\n",
    "    if n_ls_pixels == 0:\n",
    "        continue\n",
    "\n",
    "    # discard patch if it does not contain any VISIBLE landslide pixels or if it contains nodata values\n",
    "    for fn_pre in s2_pre_df['img']:\n",
    "        raster = rxr.open_rasterio(fn_pre)\n",
    "        raster = raster.assign_coords({'band' : list(raster.long_name)})\n",
    "        red = raster.sel(band=['B04']).isel(y=slice(h_start, h_end), x=slice(w_start, w_end))\n",
    "        # if there are 0 values in red band (i.e. nodata) then the patch is discarded\n",
    "        if np.any(red == 0):\n",
    "            continue\n",
    "        cloud_mask_pre = raster.sel(band='cloud_mask').isel(y=slice(h_start, h_end), x=slice(w_start, w_end))\n",
    "        cloud_mask_pre = np.isin(cloud_mask_pre, NOT_VISIBLE_CLASSES)\n",
    "\n",
    "        for fn_post in s2_post_df['img']:\n",
    "            raster = rxr.open_rasterio(fn_post)\n",
    "            raster = raster.assign_coords({'band' : list(raster.long_name)})\n",
    "            red = raster.sel(band=['B04']).isel(y=slice(h_start, h_end), x=slice(w_start, w_end))\n",
    "            # if there are 0 values in red band (i.e. nodata) then the patch is discarded\n",
    "            if np.any(red == 0):\n",
    "                continue\n",
    "            cloud_mask_post = raster.sel(band='cloud_mask').isel(y=slice(h_start, h_end), x=slice(w_start, w_end))\n",
    "            cloud_mask_post = np.isin(cloud_mask_post, NOT_VISIBLE_CLASSES)\n",
    "\n",
    "            cloud_mask_bitemporal = cloud_mask_pre | cloud_mask_post\n",
    "            perc_cloudy_pixels = cloud_mask_bitemporal.sum() / cloud_mask_bitemporal.size\n",
    "\n",
    "            vis_ls_mask = np.where(ls_mask, ~cloud_mask_bitemporal, False)\n",
    "            n_vis_ls_pixels = np.sum(vis_ls_mask)\n",
    "            if n_vis_ls_pixels == 0:\n",
    "                continue\n",
    "            \n",
    "            fn_pre_list.append(fn_pre)\n",
    "            fn_post_list.append(fn_post)\n",
    "            polygons_p_idx_with_vis_ls.append(p_idx)\n",
    "            n_vis_ls_pixels_list.append(n_vis_ls_pixels)\n",
    "            perc_cloudy_pixels_list.append(perc_cloudy_pixels)\n",
    "\n",
    "# create dataframe and save to disk\n",
    "p_df = pd.DataFrame(polygons_p_idx_with_vis_ls, columns=['h_start', 'h_end', 'w_start', 'w_end'])\n",
    "p_df['fn_pre'] = fn_pre_list\n",
    "p_df['fn_post'] = fn_post_list\n",
    "p_df['fn_dem'] = f'images/{IMAGE_FOLDER}/dem.tif'\n",
    "p_df['fn_slope'] = f'images/{IMAGE_FOLDER}/slope.tif'\n",
    "p_df['fn_aspect'] = f'images/{IMAGE_FOLDER}/aspect.tif'\n",
    "p_df['fn_poly'] = f'images/{IMAGE_FOLDER}/polygons.tif'\n",
    "p_df['landslide_pixels'] = n_vis_ls_pixels_list\n",
    "p_df['cloud_cover'] = perc_cloudy_pixels_list\n",
    "csv_path = f'images/{IMAGE_FOLDER}/patch_couples_size{PATCHSIZE}_stride{STRIDE}.csv'\n",
    "p_df.to_csv(csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
